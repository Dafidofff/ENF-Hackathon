{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equivariant Neural Field (ENF) Training\n",
    "\n",
    "This notebook contains example code for training an ENF. Most of this code is taken from the repo [enf-min-jax](https://github.com/davidknigge/enf-min-jax), which is a minimal implementation of an ENF. In that repo you can also find other examples of using ENFs for different tasks. This notebook is mainly meant to be a guide to show you how to use ENFs for your own tasks.\n",
    "\n",
    "You can use the below code to set up your environment. If you want to run the notebook on colab, most of these packages are already installed.\n",
    "\n",
    "```bash\n",
    "conda create -n enf-hackathon python=3.11\n",
    "conda activate enf-hackathon\n",
    "pip install -U \"jax[cuda12]\" flax optax matplotlib ml-collections pillow h5py tqdm jupyter wandb\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import math\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import ml_collections\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import orbax.checkpoint as ocp\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.utils.data as data\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "\n",
    "from typing import Callable, Union, Any, Sequence, Tuple, Optional\n",
    "\n",
    "# Set environment to use GPU\n",
    "os.environ['JAX_PLATFORM_NAME'] = 'gpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure dataset for colab users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import torch\n",
    "import requests\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class FigureDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for FIGURE benchmark. Downloads HDF5 files if missing.\"\"\"\n",
    "\n",
    "    BASE_URL_TEMPLATE = \"https://staff.fnwi.uva.nl/e.j.bekkers/FIGURE/{}/\"\n",
    "\n",
    "    def __init__(self, dataset_name, split=\"train\", color_consistency=1.0, data_dir=\"data/figure_datasets\", transform=None, download=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset_name (str): Name of the dataset (e.g., 'FIGURE-Shape-B').\n",
    "            split (str): Dataset split - \"train\" (default), \"test\", or \"test-bias\".\n",
    "            color_consistency (float): Color consistency level (e.g., 0.25, 0.5, 0.8, 0.9, 1.0).\n",
    "            data_dir (str): Local directory to store datasets.\n",
    "            transform (callable, optional): Transformations to apply to images.\n",
    "            download (bool): Whether to download the dataset if not found.\n",
    "        \"\"\"\n",
    "        assert split in [\"train\", \"test\", \"test-bias\"], \"Invalid split! Choose from 'train', 'test', or 'test-bias'.\"\n",
    "\n",
    "        # Convert color_consistency to a string with at least one decimal\n",
    "        consistency_str = f\"{color_consistency:.10f}\".rstrip(\"0\")\n",
    "        if consistency_str.endswith(\".\"):\n",
    "            consistency_str += \"0\"\n",
    "        self.BASE_URL = self.BASE_URL_TEMPLATE.format(consistency_str)\n",
    "\n",
    "        self.dataset_name = dataset_name\n",
    "        self.split = split\n",
    "        self.data_dir = os.path.join(data_dir, consistency_str)  # Store in a subdirectory\n",
    "        self.color_consistency = color_consistency\n",
    "\n",
    "        # Use the original filename\n",
    "        file_suffix = \"\" if split == \"train\" else f\"-{split}\"\n",
    "        self.file_name = f\"{dataset_name}{file_suffix}.h5\"\n",
    "        self.file_path = os.path.join(self.data_dir, self.file_name)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        if download and not os.path.exists(self.file_path):\n",
    "            self._download_dataset()\n",
    "\n",
    "        # Open the HDF5 file\n",
    "        self.h5file = h5py.File(self.file_path, \"r\")\n",
    "        self.images = self.h5file[\"images\"]\n",
    "        self.points_r2 = self.h5file[\"points_r2\"]\n",
    "        self.points_se2 = self.h5file[\"points_se2\"]\n",
    "        self.labels = self.h5file[\"labels\"]\n",
    "\n",
    "    def _download_dataset(self):\n",
    "        \"\"\"Downloads the dataset from the official server if it's not already available.\"\"\"\n",
    "        os.makedirs(self.data_dir, exist_ok=True)\n",
    "        url = f\"{self.BASE_URL}{self.file_name}\"\n",
    "        print(f\"Downloading {self.dataset_name} ({self.split}) from {url} to {self.file_path}...\")\n",
    "\n",
    "        response = requests.get(url, stream=True, allow_redirects=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(self.file_path, \"wb\") as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    f.write(chunk)\n",
    "            print(f\"Downloaded {self.dataset_name} ({self.split}) to {self.file_path}.\")\n",
    "        else:\n",
    "            print(f\"HTTP Status Code: {response.status_code}\")\n",
    "            print(f\"Response Headers: {response.headers}\")\n",
    "            raise RuntimeError(f\"Failed to download {self.dataset_name} ({self.split}). URL might be incorrect.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Retrieves a sample from the dataset.\"\"\"\n",
    "        image = self.images[idx]  # Image (H, W, 3)\n",
    "        points_r2 = self.points_r2[idx]  # 2D keypoints\n",
    "        points_se2 = self.points_se2[idx]  # SE(2) keypoints\n",
    "        label = self.labels[idx]  # Class label (0 = up, 1 = down)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0  # Normalize to [0,1]\n",
    "        points_r2 = torch.tensor(points_r2, dtype=torch.float32)\n",
    "        points_se2 = torch.tensor(points_se2, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        # Apply transformations (if any)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, points_r2, points_se2\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Closes the HDF5 file.\"\"\"\n",
    "        self.h5file.close()\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Ensure the file is closed when the dataset object is deleted.\"\"\"\n",
    "        self.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Define the configuration for the model, dataset, and training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define config\n",
    "config = ml_collections.ConfigDict()\n",
    "config.seed = 68\n",
    "config.debug = False\n",
    "\n",
    "# Define the ENF model parameters\n",
    "config.enf = ml_collections.ConfigDict()\n",
    "config.enf.num_in = 2           # Images are 2D\n",
    "config.enf.num_out = 3         # RGB images = 3 channels, grayscale = 1\n",
    "config.enf.num_hidden = 64\n",
    "config.enf.num_heads = 3\n",
    "config.enf.att_dim = 64\n",
    "config.enf.num_latents = 16\n",
    "config.enf.latent_dim = 32\n",
    "config.enf.emb_freq = (1.0, 2.0)   # (query, value)\n",
    "config.enf.k_nearest = 4\n",
    "config.enf.bi_invariant = \"translation\"  # Choose between translation and roto_translation_2d\n",
    "\n",
    "# Dataset config\n",
    "config.dataset = ml_collections.ConfigDict()\n",
    "config.dataset.path = \"./data\"\n",
    "config.dataset.name = \"cifar10\"           # Choose between cifar10 and FIGURE\n",
    "config.dataset.num_signals_train = 50000   # Change this to -1 for the full dataset\n",
    "config.dataset.num_signals_test = 10000\n",
    "config.dataset.batch_size = 8\n",
    "config.dataset.num_workers = 8\n",
    "\n",
    "# Specific FIGURE dataset parameters\n",
    "config.dataset.figure_type = \"FIGURE-Shape-B\"  # Choose between FIGURE-Shape-B and FIGURE-Shape-CB\n",
    "config.dataset.swap_bias = False               # Mainly interesting for down-stream tasks\n",
    "config.dataset.color_consistency = 0.9\n",
    "\n",
    "# Optimizer config\n",
    "config.optim = ml_collections.ConfigDict()\n",
    "config.optim.lr_enf = 5e-4\n",
    "config.optim.inner_lr = (1.0, 15.0, 0.0)    # (poses, context, gaussian window)\n",
    "config.optim.inner_steps = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_collate(batch: list[np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function acts as replacement to the standard PyTorch-tensor collate function in PyTorch DataLoader.\n",
    "\n",
    "    Args:\n",
    "        batch: Batch of data. Can be a numpy array, a list of numpy arrays, or nested lists of numpy arrays.\n",
    "\n",
    "    Returns:\n",
    "        Batch of data as (potential list or tuple of) numpy array(s).\n",
    "    \"\"\"\n",
    "    if isinstance(batch, np.ndarray):\n",
    "        return batch\n",
    "    elif isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "    \n",
    "\n",
    "def get_dataset(dataset_cfg):\n",
    "    \"\"\" Function which gets the dataset based on the dataset config. For now it only supports CIFAR10 and the FIGURE dataset and the double pendulum dataset.\"\"\"\n",
    "\n",
    "    if dataset_cfg.name == \"double_pendulum\":\n",
    "        from datasets.double_pendulum.double_pendulum_dataset import DoublePendulumDataset\n",
    "        train_dset = DoublePendulumDataset(dataset_cfg.path, dataset_cfg.num_signals_train)\n",
    "        test_dset = DoublePendulumDataset(dataset_cfg.path, dataset_cfg.num_signals_test)\n",
    "    elif dataset_cfg.name == \"cifar10\":\n",
    "        transform = torchvision.transforms.Compose([lambda x: np.array(x)/255])\n",
    "        train_dset = torchvision.datasets.CIFAR10(root=dataset_cfg.path, train=1, transform=transform, download=1)\n",
    "        test_dset = torchvision.datasets.CIFAR10(root=dataset_cfg.path, train=0, transform=transform, download=1)\n",
    "    elif dataset_cfg.name == \"FIGURE\":\n",
    "        from datasets.figure_dset import FigureDataset\n",
    "\n",
    "        transform = torchvision.transforms.Compose([lambda x: x.numpy(), lambda x: x.transpose(1, 2, 0)])\n",
    "        train_dset = FigureDataset(\"FIGURE-Shape-B\", split=\"train\", color_consistency=0.9, transform=transform, download=True)  \n",
    "\n",
    "        if dataset_cfg.swap_bias:\n",
    "            test_dset = FigureDataset(\"FIGURE-Shape-CB\", split=\"test-bias\", color_consistency=0.9, transform=transform, download=True)\n",
    "        else:\n",
    "            test_dset = FigureDataset(\"FIGURE-Shape-B\", split=\"test\", color_consistency=0.9, transform=transform, download=True)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset name: {dataset_cfg.name}\")\n",
    "    \n",
    "    return train_dset, test_dset\n",
    "\n",
    "\n",
    "def get_dataloader(dataset_cfg, shuffle_train=True):\n",
    "    \"\"\" Function which gets the dataloader based on the dataset config. \"\"\"\n",
    "\n",
    "    train_dset, test_dset = get_dataset(dataset_cfg)\n",
    "    \n",
    "    if dataset_cfg.num_signals_train != -1:\n",
    "        train_dset = data.Subset(train_dset, np.arange(0, dataset_cfg.num_signals_train))\n",
    "    if dataset_cfg.num_signals_test != -1:\n",
    "        test_dset = data.Subset(test_dset, np.arange(0, dataset_cfg.num_signals_test))\n",
    "\n",
    "    train_loader = data.DataLoader(\n",
    "        train_dset,\n",
    "        batch_size=dataset_cfg.batch_size,\n",
    "        shuffle=shuffle_train,\n",
    "        num_workers=dataset_cfg.num_workers,\n",
    "        collate_fn=numpy_collate,\n",
    "        persistent_workers=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    test_loader = data.DataLoader(\n",
    "        test_dset,\n",
    "        batch_size=dataset_cfg.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=dataset_cfg.num_workers,\n",
    "        collate_fn=numpy_collate,\n",
    "        persistent_workers=False,\n",
    "        drop_last=True\n",
    "    )\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "### Positional embedding\n",
    "\n",
    "The positional embedding is a key component that transforms spatial coordinates into a higher-dimensional representation. The mathematical formula is:\n",
    "\n",
    "1. First, scale the input coordinates: $\\pi(x + 1)$, we do this to shift the input coordinates from the range $[-1, 1]$ to the range $[0, 2\\pi]$\n",
    "2. Project to a lower dimension with random weights: $e = W \\cdot \\pi(x + 1)$ where $W \\sim \\mathcal{N}(0, \\text{freq})$\n",
    "3. Apply sinusoidal encoding: $\\sin([e, e + \\pi/2])$\n",
    "4. Project to final embedding dimension: $\\text{PosEmb}(x) = W_{\\text{out}} \\cdot \\sin([e, e + \\pi/2])$\n",
    "\n",
    "This embedding helps the model capture spatial relationships at different frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmb(nn.Module):\n",
    "    embedding_dim: int\n",
    "    freq: float\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, coords):\n",
    "        emb = nn.Dense(self.embedding_dim // 2, kernel_init=nn.initializers.normal(self.freq), use_bias=False)(\n",
    "            jnp.pi * (coords + 1))\n",
    "        return nn.Dense(self.embedding_dim)(jnp.sin(jnp.concatenate([emb, emb + jnp.pi / 2.0], axis=-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the positional embedding\n",
    "\n",
    "For different frequencies we visualize the positional embedding by generating a grid of coordinates and applying a random linear projection over the embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the positional embedding for different frequencies\n",
    "freqs = [1.0, 5.0, 20.0]\n",
    "posemb = [PosEmb(128, freq) for freq in freqs]\n",
    "\n",
    "# Create a grid of coordinates\n",
    "grid = jnp.stack(jnp.meshgrid(jnp.linspace(-1, 1, 100), jnp.linspace(-1, 1, 100)), axis=-1)\n",
    "grid = jnp.reshape(grid, (-1, 2))\n",
    "grid = jnp.repeat(grid[None, ...], 1, axis=0)\n",
    "params = [posemb[i].init(jax.random.PRNGKey(i), grid) for i in range(len(posemb))]\n",
    "out = [posemb[i].apply(params[i], grid) for i in range(len(posemb))]\n",
    "\n",
    "# Sample a random linear projection over the embedding dimension\n",
    "key = jax.random.PRNGKey(0)\n",
    "W = jax.random.normal(key, (128, 3))\n",
    "out = [out[i] @ W for i in range(len(posemb))]\n",
    "\n",
    "# Plot the output for each frequency\n",
    "plt.figure(figsize=(15, 4))\n",
    "for i in range(len(posemb)):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.imshow(out[i][0].reshape(100, 100, -1)) # average over the embedding dimension\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"Frequency: {freqs[i]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the bi-invariant functions\n",
    "\n",
    "Bi-invariant functions are crucial for ensuring equivariance in neural fields. They transform input coordinates and latent poses into representations that respect certain symmetries:\n",
    "\n",
    "1. **Translation Bi-invariant**:\n",
    "   - Formula: $\\mathbf{a}_{\\text{trans}}(x, p) = x - p$\n",
    "   - This creates a representation that is invariant to global translations of both input coordinates and latent poses.\n",
    "\n",
    "2. **Roto-Translation Bi-invariant (2D)**:\n",
    "   - For input coordinates $x \\in \\mathbb{R}^2$ and latent poses $p = (p_{\\text{pos}}, \\theta) \\in \\mathbb{R}^2 \\times \\mathbb{R}$:\n",
    "   - First compute relative position: $\\Delta x = x - p_{\\text{pos}}$\n",
    "   - Then rotate by $-\\theta$ to get invariant representation:\n",
    "     - $\\mathbf{a}_{\\text{roto-trans}}(x, p)_1 = \\Delta x_1 \\cos(\\theta) + \\Delta x_2 \\sin(\\theta)$\n",
    "     - $\\mathbf{a}_{\\text{roto-trans}}(x, p)_2 = -\\Delta x_1 \\sin(\\theta) + \\Delta x_2 \\cos(\\theta)$\n",
    "   - This creates a representation that is invariant to both translations and rotations.\n",
    "\n",
    "These bi-invariants ensure that the neural field's predictions remain consistent under the corresponding transformations of the input space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bi_invariant(name: str):\n",
    "    \"\"\" Get the bi-invariant function by name. Currently supports 'translation' and 'roto_translation_2d'\"\"\"\n",
    "    \n",
    "    if name == \"translation\":\n",
    "        return TranslationBI()\n",
    "    elif name == \"roto_translation_2d\":\n",
    "        return RotoTranslationBI2D()\n",
    "    else:\n",
    "        raise ValueError(f\"Bi-invariant {name} not found.\")\n",
    "\n",
    "\n",
    "class TranslationBI:   \n",
    "    def __call__(self, x: jnp.ndarray, p: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"Compute the translation equivariant bi-invariant for ND data.\n",
    "        \n",
    "        Args:\n",
    "            x: The input data. Shape (batch_size, num_coords, coord_dim).\n",
    "            p: The latent poses. Shape (batch_size, num_latents, coord_dim).\n",
    "        \"\"\"\n",
    "        return x[:, :, None, :] - p[:, None, :, :]\n",
    "\n",
    "\n",
    "class RotoTranslationBI2D:\n",
    "    def __call__(self, x: jnp.ndarray, p: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\" Compute roto-translation bi-invariant for 2D data. This assumes the input coordinates are\n",
    "        2D, i.e. do not include the orientation. The latents include orientation angle theta.\n",
    "        \n",
    "        Args:\n",
    "            x: The input data. Shape (batch_size, num_coords, pos_dim (2)).\n",
    "            p: The latent poses. Shape (batch_size, num_latents, pos_dim (2) + theta (1)).\n",
    "        \"\"\"\n",
    "        # Compute the relative position between the input and the latent poses\n",
    "        rel_pos = x[:, :, None, :2] - p[:, None, :, :2]\n",
    "\n",
    "        # Get the orientation angle theta and convert to [cos(θ), sin(θ)]\n",
    "        theta = p[:, None, :, 2:]\n",
    "        cos_theta = jnp.cos(theta)\n",
    "        sin_theta = jnp.sin(theta)\n",
    "\n",
    "        # Compute the relative orientation between the input and the latent poses\n",
    "        invariant1 = rel_pos[..., 0] * cos_theta + rel_pos[..., 1] * sin_theta\n",
    "        invariant2 = -rel_pos[..., 0] * sin_theta + rel_pos[..., 1] * cos_theta\n",
    "        return jnp.stack([invariant1, invariant2], axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Initialization\n",
    "\n",
    "The initialization of latents is a crucial step in our Equivariant Neural Field model. The `initialize_grid_positions` and `initialize_latents` functions work together to:\n",
    "\n",
    "1. Create a structured grid of latent positions that will serve as anchor points for the model\n",
    "2. Add appropriate pose information depending on the type of bi-invariance (translation or roto-translation)\n",
    "\n",
    "For translation-only bi-invariance:\n",
    "- Creates a regular grid of positions in N-dimensional space\n",
    "- Grid points are normalized to [-1, 1] range\n",
    "- Number of grid points is determined by the desired number of latents\n",
    "- Small random noise is added to prevent exact grid alignment\n",
    "\n",
    "For roto-translation bi-invariance (2D):\n",
    "- Creates a 2D grid of positions as above\n",
    "- Additionally adds a rotation angle θ for each position\n",
    "- Rotation angles are uniformly sampled from [0, 2π]\n",
    "\n",
    "These latent positions and poses act as learnable reference points that the model uses to decompose and represent the input signal in an equivariant way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_grid_positions(batch_size: int, num_latents: int, data_dim: int) -> jnp.ndarray:\n",
    "    \"\"\"Initialize a grid of positions in N dimensions.\"\"\"\n",
    "    # Calculate grid size per dimension\n",
    "    grid_size = int(math.ceil(num_latents ** (1/data_dim)))\n",
    "    lims = 1 - 1 / grid_size\n",
    "    \n",
    "    # Create linspace for each dimension\n",
    "    linspaces = [jnp.linspace(-lims, lims, grid_size) for _ in range(data_dim)]\n",
    "    \n",
    "    # Create meshgrid\n",
    "    grid = jnp.stack(jnp.meshgrid(*linspaces, indexing='ij'), axis=-1)\n",
    "    \n",
    "    # Reshape and repeat for batch\n",
    "    positions = jnp.reshape(grid, (1, -1, data_dim))\n",
    "    positions = positions.repeat(batch_size, axis=0)\n",
    "    \n",
    "    # If we have more points than needed, truncate\n",
    "    if positions.shape[1] > num_latents:\n",
    "        positions = positions[:, :num_latents, :]\n",
    "    \n",
    "    return positions\n",
    "\n",
    "def initialize_all_in_the_center(batch_size: int, num_latents: int, data_dim: int) -> jnp.ndarray:\n",
    "    \"\"\"Initialize all latents in the center of the data space.\"\"\"\n",
    "    positions = jnp.zeros((batch_size, num_latents, data_dim))\n",
    "    return positions\n",
    "\n",
    "\n",
    "def initialize_latents(\n",
    "    batch_size: int,\n",
    "    num_latents: int,\n",
    "    latent_dim: int,\n",
    "    data_dim: int,\n",
    "    bi_invariant_cls: Any,\n",
    "    key: Any,\n",
    "    window_scale: float = 2.0,\n",
    "    noise_scale: float = 0.1,\n",
    ") -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"Initialize the latent variables based on the bi-invariant type.\"\"\"\n",
    "    key, subkey = jax.random.split(key)\n",
    "\n",
    "    assert math.ceil(num_latents ** (1/data_dim)) ** data_dim == num_latents, \"Number of latents must be a perfect power of the data dimension\"\n",
    "\n",
    "    if type(bi_invariant_cls) == TranslationBI:\n",
    "        # For translation-only, positions are same dimension as data\n",
    "        pose = initialize_grid_positions(batch_size, num_latents, data_dim)\n",
    "        # pose = initialize_all_in_the_center(batch_size, num_latents, data_dim)\n",
    "\n",
    "        # Add some random noise to positions to prevent exact grid alignment\n",
    "        pose = pose + jax.random.normal(subkey, pose.shape) * noise_scale / jnp.sqrt(num_latents)\n",
    "\n",
    "    elif type(bi_invariant_cls) == RotoTranslationBI2D:\n",
    "        if data_dim != 2:\n",
    "            raise ValueError(\"RotoTranslationBI2D requires 2D data\")\n",
    "        \n",
    "        # Initialize positions in 2D\n",
    "        positions_2d = initialize_grid_positions(batch_size, num_latents, 2)\n",
    "        \n",
    "        # Add some random noise to positions to prevent exact grid alignment\n",
    "        positions_2d = positions_2d + jax.random.normal(subkey, positions_2d.shape) * noise_scale / jnp.sqrt(num_latents)\n",
    "\n",
    "        # Add orientation angle theta\n",
    "        key, subkey = jax.random.split(key)\n",
    "        theta = jax.random.uniform(subkey, (batch_size, num_latents, 1)) * 2 * jnp.pi\n",
    "        \n",
    "        # Concatenate positions and theta\n",
    "        pose = jnp.concatenate([positions_2d, theta], axis=-1)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported bi-invariant type: {bi_invariant_cls}\")\n",
    "\n",
    "    # Initialize context vectors and gaussian window\n",
    "    context = jnp.ones((batch_size, num_latents, latent_dim)) / latent_dim\n",
    "    window = jnp.ones((batch_size, num_latents, 1)) * window_scale / jnp.sqrt(num_latents)\n",
    "    \n",
    "    return pose, context, window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equivariant Neural Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equivariant Neural Fields (ENFs)\n",
    "\n",
    "ENFs are a type of neural network architecture designed to respect geometric symmetries in data while learning continuous representations. The key components are:\n",
    "\n",
    "1. **Bi-invariant Transformations**: \n",
    "   - Calculate a geometric value of the combination of the input coordinates and the latent poses that is bi-invariant under the chosen symmetry group.\n",
    "\n",
    "2. **Cross-Attention Mechanism**:\n",
    "   - Uses attention between input coordinates and learned latent points\n",
    "   - Query (Q): Positional embedding of relative positions\n",
    "   - Key (K): Learned from latent context vectors\n",
    "   - Value (V): Conditioned on both context and relative positions\n",
    "   - Attention weights are masked by a learned Gaussian window\n",
    "\n",
    "3. **Architecture Details**:\n",
    "   - Multiple attention heads for parallel processing\n",
    "   - Positional embeddings capture spatial relationships\n",
    "   - Bi-linear conditioning in value function\n",
    "   - K-nearest neighbor attention for efficiency\n",
    "   - Final MLP projects to output dimension\n",
    "\n",
    "The model learns to decompose signals into locally-supported basis functions while maintaining equivariance properties, making it particularly suitable for image and spatial data processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EquivariantNeuralField(nn.Module):\n",
    "    \"\"\" Equivariant cross attention network for the latent points, conditioned on the poses.\n",
    "\n",
    "    Args:\n",
    "        num_hidden (int): The number of hidden units.\n",
    "        num_heads (int): The number of attention heads.\n",
    "        num_out (int): The number of output coordinates.\n",
    "        emb_freq_q (float): The frequency of the positional embedding for the query.\n",
    "        emb_freq_v (float): The frequency of the positional embedding for the value function.\n",
    "        nearest_k (int): The number of nearest latents to consider.\n",
    "        bi_invariant (Callable): The bi-invariant function to use.\n",
    "    \"\"\"\n",
    "    num_hidden: int\n",
    "    att_dim: int\n",
    "    num_heads: int\n",
    "    num_out: int\n",
    "    emb_freq: Tuple[float, float]\n",
    "    nearest_k: int\n",
    "    bi_invariant: Callable\n",
    "\n",
    "    def setup(self):\n",
    "        # Positional embedding that takes in relative positions.\n",
    "        self.pos_emb_q = nn.Sequential([PosEmb(self.num_hidden, self.emb_freq[0]), nn.Dense(self.num_heads * self.att_dim)])\n",
    "        self.pos_emb_v = nn.Sequential([PosEmb(self.num_hidden, self.emb_freq[1]), nn.Dense(2 * self.num_hidden)])\n",
    "\n",
    "        # Query, key, value functions.\n",
    "        self.W_k = nn.Dense(self.num_heads * self.att_dim)\n",
    "        self.W_v = nn.Dense(self.num_hidden)\n",
    "\n",
    "        # Value bi-linear conditioning function.\n",
    "        self.v = nn.Sequential([\n",
    "            nn.Dense(self.num_hidden),\n",
    "            nn.gelu,\n",
    "            nn.Dense(self.num_heads * self.num_hidden)])\n",
    "\n",
    "        # Output layer.\n",
    "        self.W_out = nn.Dense(self.num_out)\n",
    "\n",
    "    def __call__(self, x, p, c, g):\n",
    "        \"\"\" Apply equivariant cross attention.\n",
    "\n",
    "        Args:\n",
    "            x (jax.numpy.ndarray): The input coordinates. Shape (batch_size, num_coords, coord_dim).\n",
    "            p (jax.numpy.ndarray): The latent poses. Shape (batch_size, num_latents, coord_dim).\n",
    "            c (jax.numpy.ndarray): The latent context vectors. Shape (batch_size, num_latents, latent_dim).\n",
    "            g (jax.numpy.ndarray): The window size for the gaussian window. Shape (batch_size, num_latents, 1).\n",
    "        \"\"\"\n",
    "        # Calculate bi-invariants between input coordinates and latents\n",
    "        #   bi_inv: (batch_size, num_coords, num_latents, coord_dim)\n",
    "        bi_inv = self.bi_invariant(x, p)\n",
    "        \n",
    "        # Calculate distances based on bi-invariant magnitude\n",
    "        #   zx_mag: (batch_size, num_coords, num_latents)\n",
    "        zx_mag = jnp.sum(bi_inv ** 2, axis=-1)\n",
    "        \n",
    "        # Get the indices of the nearest latents\n",
    "        #   nearest_z: (batch_size, num_coords, num_latents, nearest_k)\n",
    "        nearest_z = jnp.argsort(zx_mag, axis=-1)[:, :, :self.nearest_k, None]\n",
    "\n",
    "        # Restrict the bi-invariants and context vectors to the nearest latents\n",
    "        zx_mag = jnp.take_along_axis(zx_mag[..., None], nearest_z, axis=2)\n",
    "        bi_inv = jnp.take_along_axis(bi_inv, nearest_z, axis=2)\n",
    "        k = jnp.take_along_axis(self.W_k(c)[:, None, :, :], nearest_z, axis=2)\n",
    "        v = jnp.take_along_axis(self.W_v(c)[:, None, :, :], nearest_z, axis=2)\n",
    "        g = jnp.take_along_axis(g[:, None, :, :], nearest_z, axis=2)\n",
    "\n",
    "        # Apply bi-invariant embedding for the query transform and conditioning of the value transform\n",
    "        #   q: (batch_size, num_coords, num_latents, num_heads * att_dim)\n",
    "        q = self.pos_emb_q(bi_inv)\n",
    "        \n",
    "        # Split the value embedding into bias and gaussian window\n",
    "        #   b_v: (batch_size, num_coords, num_latents, num_hidden)\n",
    "        #   g_v: (batch_size, num_coords, num_latents, 1)\n",
    "        b_v, g_v = jnp.split(self.pos_emb_v(bi_inv), 2, axis=-1)\n",
    "        \n",
    "        # Apply the value bi-linear conditioning function\n",
    "        #   v: (batch_size, num_coords, num_latents, num_heads * num_hidden)\n",
    "        v = self.v(v * (1 + b_v) + g_v)\n",
    "\n",
    "        # Reshape to separate the heads\n",
    "        #   q: (batch_size, num_coords, num_latents, num_heads, att_dim)\n",
    "        #   k: (batch_size, num_coords, num_latents, num_heads, att_dim)\n",
    "        #   v: (batch_size, num_coords, num_latents, num_heads, num_hidden)\n",
    "        q = q.reshape(q.shape[:-1] + (self.num_heads, -1))\n",
    "        k = k.reshape(k.shape[:-1] + (self.num_heads, -1))\n",
    "        v = v.reshape(v.shape[:-1] + (self.num_heads, -1))\n",
    "\n",
    "        # Calculate the attention weights, apply gaussian mask based on bi-invariant magnitude, broadcasting over heads.\n",
    "        #   att_logits: (batch_size, num_coords, num_latents, num_heads)\n",
    "        att_logits = ((q * k).sum(axis=-1, keepdims=True) / self.att_dim) - ((1 / g ** 2) * zx_mag)[..., None, :]\n",
    "        att = jax.nn.softmax(att_logits, axis=2)\n",
    "\n",
    "        # Attend the values to the queries and keys.\n",
    "        #   y: (batch_size, num_coords, num_latents, num_heads, num_hidden)\n",
    "        y = (att * v).sum(axis=2)\n",
    "\n",
    "        # Combine the heads and apply the output layer.\n",
    "        #   y: (batch_size, num_coords, num_latents, num_heads * num_hidden)\n",
    "        y = y.reshape(y.shape[:-2] + (-1,))\n",
    "        return self.W_out(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset, get sample image, create corresponding coordinates\n",
    "train_dloader, test_dloader = get_dataloader(config.dataset)\n",
    "sample = next(iter(train_dloader))\n",
    "if isinstance(sample, tuple) or isinstance(sample, list):\n",
    "    img_shape = sample[0].shape[1:]\n",
    "else:\n",
    "    img_shape = sample.shape[1:]\n",
    "# Random key for JAX operations\n",
    "key = jax.random.PRNGKey(55)\n",
    "\n",
    "# Create coordinate grid\n",
    "x = jnp.stack(jnp.meshgrid(jnp.linspace(-1, 1, img_shape[0]), jnp.linspace(-1, 1, img_shape[1])), axis=-1)\n",
    "x = jnp.reshape(x, (-1, 2))\n",
    "x = jnp.repeat(x[None, ...], config.dataset.batch_size, axis=0)\n",
    "\n",
    "# Define the model\n",
    "bi_inv = get_bi_invariant(config.enf.bi_invariant)\n",
    "model = EquivariantNeuralField(\n",
    "    num_hidden=config.enf.num_hidden,\n",
    "    att_dim=config.enf.att_dim,\n",
    "    num_heads=config.enf.num_heads,\n",
    "    num_out=config.enf.num_out,\n",
    "    emb_freq=config.enf.emb_freq,\n",
    "    nearest_k=config.enf.k_nearest,\n",
    "    bi_invariant=bi_inv,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model Parameters\n",
    "\n",
    "In JAX, we need to initialize model parameters by passing dummy inputs through the model so it can infer parameter shapes for JIT compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy latents for model initialization\n",
    "dummy_z = initialize_latents(\n",
    "    batch_size=config.dataset.batch_size,\n",
    "    num_latents=config.enf.num_latents,\n",
    "    latent_dim=config.enf.latent_dim,\n",
    "    data_dim=config.enf.num_in,\n",
    "    bi_invariant_cls=bi_inv,\n",
    "    key=key,\n",
    ")\n",
    "enf_params = model.init(key, x, *dummy_z)\n",
    "\n",
    "# Define optimizer for the ENF backbone\n",
    "enf_optimizer = optax.adam(learning_rate=config.optim.lr_enf)\n",
    "enf_opt_state = enf_optimizer.init(enf_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Functions\n",
    "\n",
    "Define the inner loop and outer step functions for training. The inner loop optimizes the latent variables, while the outer step updates the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def inner_loop(enf_params, coords, img, key):\n",
    "    z = initialize_latents(\n",
    "        batch_size=config.dataset.batch_size,\n",
    "        num_latents=config.enf.num_latents,\n",
    "        latent_dim=config.enf.latent_dim,\n",
    "        data_dim=config.enf.num_in,\n",
    "        bi_invariant_cls=bi_inv,\n",
    "        key=key,\n",
    "    )\n",
    "\n",
    "    def mse_loss(z):\n",
    "        out = model.apply(enf_params, coords, *z)\n",
    "        return jnp.sum(jnp.mean((out - img) ** 2, axis=(1, 2)), axis=0)\n",
    "\n",
    "    def inner_step(z):\n",
    "        # Calculate the gradient of the loss function\n",
    "        _, grads = jax.value_and_grad(mse_loss)(z)\n",
    "         \n",
    "        # Gradient descent update\n",
    "        z = jax.tree.map(lambda z, grad, lr: z - lr * grad, z, grads, config.optim.inner_lr)\n",
    "        return z\n",
    "    \n",
    "    # Perform inner loop optimization\n",
    "    for _ in range(config.optim.inner_steps):\n",
    "        z = inner_step(z)\n",
    "\n",
    "    return mse_loss(z), z\n",
    "\n",
    "@jax.jit\n",
    "def outer_step(coords, img, enf_params, enf_opt_state, key):\n",
    "    # Perform inner loop optimization\n",
    "    key, subkey = jax.random.split(key)\n",
    "    (loss, z), grads = jax.value_and_grad(inner_loop, has_aux=True)(enf_params, coords, img, key)\n",
    "\n",
    "    # Update the ENF backbone\n",
    "    enf_grads, enf_opt_state = enf_optimizer.update(grads, enf_opt_state)\n",
    "    enf_params = optax.apply_updates(enf_params, enf_grads)\n",
    "\n",
    "    # Sample new key\n",
    "    return (loss, z), enf_params, enf_opt_state, subkey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Execute the training loop, periodically logging results and saving checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_figures = True # Set to False to not save figures\n",
    "num_epochs =32\n",
    "log_interval = 500\n",
    "\n",
    "fig_path = \"figures/\"\n",
    "os.makedirs(fig_path, exist_ok=True)\n",
    "\n",
    "def plot_images(img, img_r, z, epoch, glob_step):\n",
    "        \n",
    "        # Min max normalization\n",
    "        min_val = jnp.min(img[0])\n",
    "        max_val = jnp.max(img[0])\n",
    "\n",
    "        img_r = (img_r - jnp.min(img_r)) / (jnp.max(img_r) - jnp.min(img_r))\n",
    "        img = (img - min_val) / (max_val - min_val)\n",
    "\n",
    "        # Plot the original and reconstructed image\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(jnp.reshape(img[0], (img_shape)))\n",
    "        plt.title(\"Original\")\n",
    "        plt.axis('off')\n",
    "        plt.subplot(132)\n",
    "        plt.imshow(jnp.reshape(img_r, (img_shape)))\n",
    "        plt.title(\"Reconstructed\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Plot the poses\n",
    "        plt.subplot(133)\n",
    "        plt.imshow(jnp.reshape(img_r, (img_shape)))\n",
    "        plt.title(\"Poses\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Poses are [-1, 1], map to [0, img_shape]\n",
    "        poses_m = (z[0] + 1) / 2 * img_shape[0]\n",
    "        plt.scatter(poses_m[0, :, 0], poses_m[0, :, 1], c='r')\n",
    "        plt.savefig(f\"figures/epoch_{epoch}_step_{glob_step}.png\")\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the untrained model\n",
    "if log_figures:\n",
    "    z = initialize_latents(\n",
    "        batch_size=config.dataset.batch_size,\n",
    "        num_latents=config.enf.num_latents,\n",
    "        latent_dim=config.enf.latent_dim,\n",
    "        data_dim=config.enf.num_in,\n",
    "        bi_invariant_cls=bi_inv,\n",
    "        key=key,\n",
    "    )\n",
    "    img = next(iter(train_dloader))\n",
    "    if isinstance(img, tuple) or isinstance(img, list):\n",
    "        img = img[0]\n",
    "    img_r = model.apply(enf_params, x, *z)[0]\n",
    "    plot_images(img, img_r, z, 0, 0)\n",
    "    print(\"saved figure\")\n",
    "\n",
    "glob_step, lowest_loss = 0, jnp.inf\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = []\n",
    "    with tqdm(enumerate(train_dloader), total=len(train_dloader), desc=f'Epoch {epoch}') as pbar:\n",
    "        for i, batch in pbar:\n",
    "            if isinstance(batch, tuple) or isinstance(batch, list):\n",
    "                img = batch[0]\n",
    "            else:\n",
    "                img = batch\n",
    "            y = jnp.reshape(img, (img.shape[0], -1, img.shape[-1]))\n",
    "\n",
    "            # Perform outer loop optimization\n",
    "            (loss, z), enf_params, enf_opt_state, key = outer_step(\n",
    "                x, y, enf_params, enf_opt_state, key)\n",
    "\n",
    "            epoch_loss.append(loss)\n",
    "            glob_step += 1\n",
    "\n",
    "            # Update progress bar with current loss\n",
    "            pbar.set_postfix({\"loss\": float(loss)})\n",
    "\n",
    "            if glob_step % log_interval == 0 and log_figures:\n",
    "                img_r = model.apply(enf_params, x, *z)[0]\n",
    "                plot_images(img, img_r, z, epoch, glob_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpBlock(nn.Module):\n",
    "    mlp_dim: int\n",
    "    out_dim: Optional[int] = None\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        actual_out_dim = inputs.shape[-1] if self.out_dim is None else self.out_dim\n",
    "        x = nn.Dense(\n",
    "                features=self.mlp_dim,\n",
    "                kernel_init=nn.initializers.xavier_uniform(),\n",
    "                bias_init=nn.initializers.normal(stddev=1e-6))(inputs)\n",
    "        x = nn.gelu(x)\n",
    "        output = nn.Dense(\n",
    "                features=actual_out_dim,\n",
    "                kernel_init=nn.initializers.xavier_uniform(),\n",
    "                bias_init=nn.initializers.normal(stddev=1e-6))(x)\n",
    "        return output\n",
    "    \n",
    "def modulate(x, shift, scale):\n",
    "    return x * (1 + scale[:, None]) + shift[:, None]\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    hidden_size: int\n",
    "    num_heads: int\n",
    "    mlp_ratio: float = 4.0\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x_norm = nn.LayerNorm(use_bias=False, use_scale=False)(x)\n",
    "        attn_x = nn.MultiHeadDotProductAttention(kernel_init=nn.initializers.xavier_uniform(),\n",
    "            num_heads=self.num_heads)(x_norm, x_norm)\n",
    "        x = x + attn_x\n",
    "\n",
    "        # MLP Residual.\n",
    "        x_norm2 = nn.LayerNorm(use_bias=False, use_scale=False)(x)\n",
    "        mlp_x = MlpBlock(mlp_dim=int(self.hidden_size * self.mlp_ratio))(x_norm2)\n",
    "        x = x + mlp_x\n",
    "        return x\n",
    "    \n",
    "class FinalLayer(nn.Module):\n",
    "    out_channels: int\n",
    "    hidden_size: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.LayerNorm()(x)\n",
    "        x = nn.Dense(self.out_channels, kernel_init=nn.initializers.constant(0))(x)\n",
    "        return x\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer model.\n",
    "    \"\"\"\n",
    "    hidden_size: int\n",
    "    depth: int\n",
    "    num_heads: int\n",
    "    mlp_ratio: float\n",
    "    num_classes: int\n",
    "    emb_freq: float = 1.0\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, p_0, c_0, g_0):\n",
    "        # Embed patches and poses.\n",
    "        pos_embed = PosEmb(self.hidden_size, freq=self.emb_freq)(p_0)\n",
    "        c = nn.Dense(self.hidden_size, kernel_init=nn.initializers.xavier_uniform())(c_0)\n",
    "        c = c + pos_embed\n",
    "\n",
    "        # Run DiT blocks on input and conditioning.\n",
    "        for _ in range(self.depth):\n",
    "            c = TransformerBlock(self.hidden_size, self.num_heads, self.mlp_ratio)(c)\n",
    "\n",
    "        # Final layer.\n",
    "        return FinalLayer(self.num_classes, self.hidden_size)(jnp.mean(c, axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Transformer Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_config = ml_collections.ConfigDict()\n",
    "classifier_config.num_hidden = 768\n",
    "classifier_config.depth = 12\n",
    "classifier_config.num_heads = 12\n",
    "classifier_config.mlp_ratio = 4\n",
    "classifier_config.num_classes = 10\n",
    "classifier_config.emb_freq = 1.0\n",
    "\n",
    "classifier_config.lr_transformer = 1e-4\n",
    "classifier_config.num_epochs = 32\n",
    "classifier_config.log_interval = 500\n",
    "\n",
    "transformer = TransformerClassifier(\n",
    "    hidden_size=classifier_config.num_hidden,\n",
    "    depth=classifier_config.depth,\n",
    "    num_heads=classifier_config.num_heads,\n",
    "    mlp_ratio=classifier_config.mlp_ratio,\n",
    "    num_classes=classifier_config.num_classes,\n",
    "    )\n",
    "\n",
    "# Create dummy latents for model init\n",
    "key, subkey = jax.random.split(key)\n",
    "temp_z = initialize_latents(\n",
    "    batch_size=1,  # Only need one example for initialization\n",
    "    num_latents=config.enf.num_latents,\n",
    "    latent_dim=config.enf.latent_dim,\n",
    "    data_dim=config.enf.num_in,\n",
    "    bi_invariant_cls=TranslationBI(),\n",
    "    key=subkey,\n",
    ")\n",
    "\n",
    "# Define optimizer for the transformer model\n",
    "transformer_opt = optax.adam(learning_rate=classifier_config.lr_transformer)\n",
    "transformer_params = transformer.init(key, *temp_z)\n",
    "transformer_opt_state = transformer_opt.init(transformer_params)\n",
    "\n",
    "# Training the classifier is much faster when the context vectors are normalized.\n",
    "# We determine mean and std of the context vectors and normalize them.\n",
    "c_list = []\n",
    "for i, (img, _) in enumerate(train_dloader):\n",
    "    # Do inner loop, get latents\n",
    "    y = jnp.reshape(img, (img.shape[0], -1, img.shape[-1]))\n",
    "    _, z = inner_loop(enf_params, x, y, key)\n",
    "\n",
    "    # Append context vectors to list\n",
    "    c_list.append(z[1])\n",
    "\n",
    "# Compute mean and std of the context vectors\n",
    "c_mean = jnp.mean(jnp.concatenate(c_list, axis=0), axis=(0, 1))\n",
    "c_std = jnp.std(jnp.concatenate(c_list, axis=0), axis=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Transformer Classifier Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def classifier_outer_step(coords, img, labels, transformer_params, transformer_opt_state, key):\n",
    "    # Perform inner loop optimization to get latents\n",
    "    key, subkey = jax.random.split(key)\n",
    "    _, z = inner_loop(enf_params, coords, img, key)\n",
    "\n",
    "    # Normalize context vectors\n",
    "    p, c, g = z\n",
    "    c = (c - c_mean) / c_std\n",
    "    z = (p, c, g)\n",
    "\n",
    "    def cross_entropy_loss(params):\n",
    "        # Get transformer predictions from latents\n",
    "        logits = transformer.apply(params, *z)\n",
    "        # Compute cross entropy loss\n",
    "        one_hot = jax.nn.one_hot(labels, num_classes=10)\n",
    "        loss = -jnp.sum(one_hot * jax.nn.log_softmax(logits), axis=-1)\n",
    "        return jnp.mean(loss)\n",
    "\n",
    "    # Get gradients\n",
    "    loss, grads = jax.value_and_grad(cross_entropy_loss)(transformer_params)\n",
    "\n",
    "    # Update transformer parameters\n",
    "    updates, transformer_opt_state = transformer_opt.update(grads, transformer_opt_state)\n",
    "    transformer_params = optax.apply_updates(transformer_params, updates)\n",
    "\n",
    "    return (loss, z), transformer_params, transformer_opt_state, subkey\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for the transformer classifier\n",
    "for epoch in tqdm(range(classifier_config.num_epochs), desc=\"Training classifier\"):\n",
    "    epoch_loss = []\n",
    "    for i, (img, labels) in enumerate(train_dloader):\n",
    "        y = jnp.reshape(img, (img.shape[0], -1, img.shape[-1]))\n",
    "\n",
    "        # Optimize the transformer classifier\n",
    "        (loss, z), transformer_params, transformer_opt_state, key = classifier_outer_step(\n",
    "            x, y, labels, transformer_params, transformer_opt_state, key)\n",
    "\n",
    "        epoch_loss.append(loss)\n",
    "        glob_step += 1\n",
    "\n",
    "        if glob_step % classifier_config.log_interval == 0:\n",
    "            # Get predictions on a batch\n",
    "            logits = transformer.apply(transformer_params, *z)\n",
    "            preds = jnp.argmax(logits, axis=-1)\n",
    "            accuracy = jnp.mean(preds == labels)\n",
    "\n",
    "            tqdm.write(f\"CLASSIFIER ep {epoch} / step {glob_step} || \" \n",
    "                        f\"loss: {sum(epoch_loss) / len(epoch_loss):.4f}, \"\n",
    "                        f\"accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "for i, (img, labels) in enumerate(test_dloader):\n",
    "    y = jnp.reshape(img, (img.shape[0], -1, img.shape[-1]))\n",
    "    _, z = inner_loop(enf_params, x, y, key)\n",
    "    # Normalize context vectors\n",
    "    p, c, g = z\n",
    "    c = (c - c_mean) / c_std\n",
    "    z = (p, c, g)\n",
    "    logits = transformer.apply(transformer_params, *z)\n",
    "    preds = jnp.argmax(logits, axis=-1)\n",
    "    accuracy = jnp.mean(preds == labels)\n",
    "    accs.append(accuracy)\n",
    "print(f\"TEST accuracy: {sum(accs) / len(accs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enf-hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
