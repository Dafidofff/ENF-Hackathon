{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to JAX/Flax/Optax\n",
    "\n",
    "This notebook covers the essential concepts needed to understand the ENF codebase. We'll focus on the key features and patterns used throughout the code.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### 1. JAX Basics\n",
    "\n",
    "JAX is NumPy on steroids - it provides automatic differentiation and compilation to accelerators (GPU/TPU). Key features used in our codebase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# 1. jit compilation - makes functions run faster by compiling to XLA\n",
    "@jax.jit\n",
    "def fast_function(x):\n",
    "    return jnp.sum(x ** 2)\n",
    "\n",
    "# 2. Automatic differentiation using jax.grad (to return only the gradient)\n",
    "# and jax.value_and_grad (to return the function value/loss and gradient)\n",
    "@jax.jit\n",
    "def loss_and_grad(x):\n",
    "    loss = jnp.sum(x ** 2)\n",
    "    grad = jax.grad(lambda x: loss)(x)\n",
    "    return loss, grad\n",
    "\n",
    "# 3. Random number handling - JAX requires explicit PRNG key management\n",
    "key = jax.random.PRNGKey(42)\n",
    "key, subkey = jax.random.split(key)  # Get new keys for random operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because jit compiles the function to XLA, it needs to be a \"pure\" function.\n",
    "# \"Pure\" functions are functions that do not have side effects and always \n",
    "# return the same output for the same input. Easiest way to ensure this is to\n",
    "# write functions that only operate on their arguments.\n",
    "y = 10.0\n",
    "\n",
    "@jax.jit\n",
    "def fast_function(x):\n",
    "    x = y + 10.0\n",
    "    return jnp.sum(x ** 2) + y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Flax Neural Networks\n",
    "\n",
    "Flax is a neural network library built on JAX. It uses a class-based approach with the `nn.Module` system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    hidden_size: int  # Configuration parameters are class attributes\n",
    "    \n",
    "    def setup(self):\n",
    "        self.dense1 = nn.Dense(self.hidden_size)\n",
    "        self.dense2 = nn.Dense(1)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = nn.relu(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "# Initialize model and parameters\n",
    "model = SimpleNN(hidden_size=64)\n",
    "key = jax.random.PRNGKey(0)\n",
    "x = jnp.ones((1, 10))\n",
    "params = model.init(key, x)  # Returns initialized parameters\n",
    "\n",
    "# Applying the model requires passing the parameters\n",
    "output = model.apply(params, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also allows for stateful modules, which are useful for recurrent neural networks and other sequential models. This is done by using the `@nn.compact` decorator, like below. In our repo, we mostly use the `setup` method to define the layers as its a bit more explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    hidden_size: int  # Configuration parameters are class attributes\n",
    "    \n",
    "    @nn.compact  # Makes the module stateful\n",
    "    def __call__(self, x):\n",
    "        # Layers are created on first call\n",
    "        x = nn.Dense(self.hidden_size)(x)\n",
    "        x = nn.relu(x)\n",
    "        return nn.Dense(1)(x)\n",
    "\n",
    "# Initialize model and parameters\n",
    "model = SimpleNN(hidden_size=64)\n",
    "key = jax.random.PRNGKey(0)\n",
    "x = jnp.ones((1, 10))\n",
    "params = model.init(key, x)  # Returns initialized parameters\n",
    "\n",
    "# Applying the model requires passing the parameters\n",
    "output = model.apply(params, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Optax Optimizers\n",
    "\n",
    "Optax provides optimizers and gradient transformation utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optax.adam(learning_rate=1e-3)\n",
    "opt_state = optimizer.init(params)  # Initialize optimizer state\n",
    "\n",
    "# Training step\n",
    "@jax.jit\n",
    "def train_step(params, opt_state, batch):\n",
    "    def loss_fn(params):\n",
    "        output = model.apply(params, batch)\n",
    "        return jnp.mean(output ** 2)\n",
    "    \n",
    "    # Get loss and gradients\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "    \n",
    "    # Update parameters\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    \n",
    "    return params, opt_state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Key Patterns in the ENF Codebase\n",
    "\n",
    "1. **Model Structure**: Models are defined as Flax modules (like `EquivariantNeuralField`)\n",
    "2. **Training Loop Pattern**:\n",
    "   - Inner loop optimization (for meta-learning)\n",
    "   - Outer loop updates model parameters\n",
    "3. **Functional Updates**: Everything is immutable - new states/parameters are returned rather than modified in-place\n",
    "\n",
    "### Common Gotchas\n",
    "\n",
    "1. JAX arrays are immutable - operations return new arrays\n",
    "2. `@jax.jit` functions must be pure (same inputs â†’ same outputs)\n",
    "3. Random operations need explicit PRNG key management\n",
    "4. Shape errors can be cryptic - use `jax.debug.print()` for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "class MetaLearningExample(nn.Module):\n",
    "    hidden_size: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, z):\n",
    "        # x: input data\n",
    "        # z: latent variables to be optimized in inner loop\n",
    "        combined = jnp.concatenate([x, z], axis=-1)\n",
    "        return nn.Dense(1)(nn.relu(nn.Dense(self.hidden_size)(combined)))\n",
    "\n",
    "# Initialize model\n",
    "model = MetaLearningExample(hidden_size=64)\n",
    "params = model.init(key, x, jnp.ones((1, 5)))\n",
    "\n",
    "# Create optimizer for outer loop (model parameters)\n",
    "outer_optimizer = optax.adam(learning_rate=1e-3)\n",
    "outer_opt_state = outer_optimizer.init(params)\n",
    "\n",
    "@jax.jit\n",
    "def inner_loop(params, x, y, z, inner_steps=3, inner_lr=0.1):\n",
    "    \"\"\"Optimize latent variables z to fit current data.\"\"\"\n",
    "    def loss_fn(z):\n",
    "        pred = model.apply(params, x, z)\n",
    "        return jnp.mean((pred - y) ** 2)\n",
    "    \n",
    "    def inner_step(z, _):\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(z)\n",
    "        z = z - inner_lr * grads  # Simple gradient descent\n",
    "        return z, loss\n",
    "    \n",
    "    # Run inner optimization loop\n",
    "    z, losses = jax.lax.scan(inner_step, z, None, length=inner_steps)\n",
    "    \n",
    "    return loss_fn(z), z\n",
    "\n",
    "@jax.jit\n",
    "def outer_step(params, opt_state, x, y, z):\n",
    "    \"\"\"Update model parameters using meta-gradients.\"\"\"\n",
    "    def meta_loss(params):\n",
    "        loss, optimal_z = inner_loop(params, x, y, z)\n",
    "        return loss\n",
    "    \n",
    "    # Get meta-gradients\n",
    "    loss, grads = jax.value_and_grad(meta_loss)(params)\n",
    "    \n",
    "    # Update model parameters\n",
    "    updates, opt_state = outer_optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    \n",
    "    return params, opt_state, loss\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        # Initialize latent variables for this batch\n",
    "        z = jnp.zeros((batch_x.shape[0], 5))\n",
    "        \n",
    "        # Outer loop update\n",
    "        params, outer_opt_state, loss = outer_step(params, outer_opt_state, batch_x, batch_y, z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enf-hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
